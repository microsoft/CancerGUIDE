{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11090a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from dotenv) (1.1.0)\n",
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/alyssa-test/code/rwep_experiments/jsam/mtb_sample_data/libs/sam_mtb_utils\n",
      "Obtaining file:///mnt/batch/tasks/shared/LS_root/mounts/clusters/alyssa-test/code/rwep_experiments/jsam/mtb_sample_data/libs/sam_mtb_utils\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: openai in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sam_mtb_utils==0.1.0) (1.97.1)\n",
      "Requirement already satisfied: tenacity in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sam_mtb_utils==0.1.0) (9.1.2)\n",
      "Requirement already satisfied: langchain-openai in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sam_mtb_utils==0.1.0) (0.3.28)\n",
      "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sam_mtb_utils==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sam_mtb_utils==0.1.0) (2.9.2)\n",
      "Requirement already satisfied: azure-identity in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sam_mtb_utils==0.1.0) (1.21.0)\n",
      "Requirement already satisfied: tiktoken in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sam_mtb_utils==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pydantic>=2.0.0->sam_mtb_utils==0.1.0) (4.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pydantic>=2.0.0->sam_mtb_utils==0.1.0) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pydantic>=2.0.0->sam_mtb_utils==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-identity->sam_mtb_utils==0.1.0) (1.32.3)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-identity->sam_mtb_utils==0.1.0) (1.33.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-identity->sam_mtb_utils==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-identity->sam_mtb_utils==0.1.0) (38.0.4)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from langchain-openai->sam_mtb_utils==0.1.0) (0.3.72)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from openai->sam_mtb_utils==0.1.0) (0.28.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from openai->sam_mtb_utils==0.1.0) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from openai->sam_mtb_utils==0.1.0) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from openai->sam_mtb_utils==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from openai->sam_mtb_utils==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from tiktoken->sam_mtb_utils==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from tiktoken->sam_mtb_utils==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: idna>=2.8 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai->sam_mtb_utils==0.1.0) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai->sam_mtb_utils==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core>=1.31.0->azure-identity->sam_mtb_utils==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cryptography>=2.5->azure-identity->sam_mtb_utils==0.1.0) (1.17.1)\n",
      "Requirement already satisfied: certifi in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai->sam_mtb_utils==0.1.0) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai->sam_mtb_utils==0.1.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->sam_mtb_utils==0.1.0) (0.16.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai->sam_mtb_utils==0.1.0) (25.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai->sam_mtb_utils==0.1.0) (0.4.8)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai->sam_mtb_utils==0.1.0) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai->sam_mtb_utils==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msal>=1.30.0->azure-identity->sam_mtb_utils==0.1.0) (2.4.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msal-extensions>=1.2.0->azure-identity->sam_mtb_utils==0.1.0) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->sam_mtb_utils==0.1.0) (1.26.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->sam_mtb_utils==0.1.0) (3.4.1)\n",
      "Requirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity->sam_mtb_utils==0.1.0) (2.22)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai->sam_mtb_utils==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai->sam_mtb_utils==0.1.0) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai->sam_mtb_utils==0.1.0) (3.11.1)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai->sam_mtb_utils==0.1.0) (0.23.0)\n",
      "Building wheels for collected packages: sam_mtb_utils\n",
      "  Building editable for sam_mtb_utils (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sam_mtb_utils: filename=sam_mtb_utils-0.1.0-0.editable-py3-none-any.whl size=2037 sha256=d7ffc77aaf5af764fd54f006bd2255ad5e1818b7ab60921847058e001d7b74ee\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pdx4ljv3/wheels/66/b4/58/e314f37d8fadc3229a2a97dd2e52adce692e94c0dab12e7677\n",
      "Successfully built sam_mtb_utils\n",
      "Installing collected packages: sam_mtb_utils\n",
      "  Attempting uninstall: sam_mtb_utils\n",
      "    Found existing installation: sam_mtb_utils 0.1.0\n",
      "    Uninstalling sam_mtb_utils-0.1.0:\n",
      "      Successfully uninstalled sam_mtb_utils-0.1.0\n",
      "Successfully installed sam_mtb_utils-0.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install pydicom\n",
    "# !pip install matplotlib\n",
    "# !pip install pillow\n",
    "# !pip install dotenv\n",
    "\n",
    "\n",
    "# %cd /home/azureuser/cloudfiles/code/rwep_experiments/jsam/mtb_sample_data/libs/sam_mtb_utils/\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6c4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_parquet_from_directory(parquet_dir):\n",
    "    \"\"\"\n",
    "    Reads the Parquet file from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base path where the directories are located.\n",
    "        directory_name (str): The name of the directory containing the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The data from the Parquet file as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    file_frames = []\n",
    "    # parquet_dir = base_path / directory_name\n",
    "    for file in Path(parquet_dir).glob(\"*.parquet\"):\n",
    "        df = pd.read_parquet(file)\n",
    "        file_frames.append(df)\n",
    "    if file_frames:\n",
    "        return pd.concat(file_frames, ignore_index=True)\n",
    "    raise FileNotFoundError(f\"No Parquet files found in {parquet_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf893b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients with non-small cell lung cancer: 1464\n"
     ]
    }
   ],
   "source": [
    "diagnosis = \"/home/azureuser/data/rwesdataforge/protege/2025-06-16-clinical_cohort_2196total+mtb_pathnotes/protege/clinical-cohort/structured-ehr/DD48_DIAGNOSIS\"\n",
    "df = read_parquet_from_directory(diagnosis)\n",
    "lung_patients = df[df['DX_CODE'] == 'C34']\n",
    "\n",
    "# Then exclude small cell lung cancer from histology\n",
    "nsclc_patients1 = lung_patients[\n",
    "    ~lung_patients['HISTOLOGY'].str.contains('small cell', case=False, na=False)\n",
    "]\n",
    "\n",
    "print(f\"Number of patients with non-small cell lung cancer: {len(nsclc_patients1)}\")\n",
    "target_patient_ids = nsclc_patients1['PTID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492a4688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Parquet files from /home/azureuser/data/rwesdataforge/protege/2025-06-16-clinical_cohort_2196total+mtb_pathnotes/protege/clinical-cohort/structured-ehr/DD48_NOTES\n",
      "Successfully read NOTES data: 1451 rows\n",
      "Reading Parquet files from /home/azureuser/data/rwesdataforge/protege/2025-06-16-clinical_cohort_2196total+mtb_pathnotes/protege/clinical-cohort/structured-ehr/DD48_TREATMENT\n",
      "Successfully read TREATMENT data: 1451 rows\n"
     ]
    }
   ],
   "source": [
    "parquet_dir=\"/home/azureuser/data/rwesdataforge/protege/2025-06-16-clinical_cohort_2196total+mtb_pathnotes/protege/clinical-cohort/structured-ehr/\"\n",
    "\n",
    "# Initialize variables for NOTES and TREATMENT dataframes\n",
    "NOTES = None\n",
    "TREATMENT = None\n",
    "\n",
    "if not Path(parquet_dir).exists():\n",
    "    raise FileNotFoundError(f\"The directory {parquet_dir} does not exist.\")\n",
    "\n",
    "for subdir in Path(parquet_dir).iterdir():\n",
    "    if subdir.is_dir():\n",
    "        name = str(subdir).split('/')[-1]\n",
    "        \n",
    "        # Only process NOTES and TREATMENT directories\n",
    "        if \"NOTES\" not in name and \"TREATMENT\" not in name:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Reading Parquet files from {subdir}\")\n",
    "        try:\n",
    "            file_frames = read_parquet_from_directory(subdir)\n",
    "            # Filter the DataFrame to include only target patients\n",
    "            file_frames = file_frames[file_frames['PTID'].isin(target_patient_ids)]\n",
    "            \n",
    "            # Collapse the dataframe so that there is only one row per patient, combining all columns   \n",
    "            file_frames = file_frames.groupby('PTID').agg(\n",
    "                lambda x: [val for val in x.dropna() if pd.notna(val)] or np.nan\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Add file name to the data frame columns as an \"_name\" suffix\n",
    "            file_frames.columns = [f\"{col}_{name}\" if col != 'PTID' else col for col in file_frames.columns]\n",
    "            \n",
    "            if file_frames.empty:\n",
    "                print(f\"No relevant data found in {subdir} for target patients.\")\n",
    "                continue\n",
    "            \n",
    "            # Store the specific dataframes we want\n",
    "            if \"TREATMENT\" in name:\n",
    "                TREATMENT = file_frames.copy()\n",
    "                print(f\"Successfully read TREATMENT data: {len(file_frames)} rows\")\n",
    "            elif \"NOTES\" in name:\n",
    "                NOTES = file_frames.copy()\n",
    "                print(f\"Successfully read NOTES data: {len(file_frames)} rows\")\n",
    "                \n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "\n",
    "# Verify we got both dataframes\n",
    "if NOTES is None:\n",
    "    print(\"Warning: NOTES dataframe was not found\")\n",
    "if TREATMENT is None:\n",
    "    print(\"Warning: TREATMENT dataframe was not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b391fc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä CLINICAL NOTES ANALYSIS\n",
      "============================================================\n",
      "üîç Analyzing notes from 1451 patients with available note data\n",
      "\n",
      "üìà SUMMARY STATISTICS:\n",
      "----------------------------------------\n",
      "  Total Patients      : 1451\n",
      "  Mean Notes/Patient  : 61.50\n",
      "  Std Deviation       : 57.63\n",
      "  Minimum Notes       : 1\n",
      "  25th Percentile     : 23.0\n",
      "  Median Notes        : 46.0\n",
      "  75th Percentile     : 80.0\n",
      "  Maximum Notes       : 469\n",
      "  Patients w/ Zero Notes: 0 (0.0%)\n",
      "\n",
      "üí° KEY INSIGHTS:\n",
      "----------------------------------------\n",
      "  Total Notes Across All Patients: 89,234\n",
      "  Patients with Notes Available: 1451 (100.0%)\n",
      "  Avg Notes (excluding zero counts): 61.50\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ANALYSIS OF TOTAL AMOUNT OF NOTES IN A RECORD\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä CLINICAL NOTES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "notes_series = NOTES[\"DEID_NOTE_TXT_DD48_NOTES\"].dropna()\n",
    "print(f\"üîç Analyzing notes from {len(notes_series)} patients with available note data\")\n",
    "\n",
    "# Count number of non-empty, stripped items in each comma-separated string\n",
    "note_lengths = notes_series.apply(lambda x: len([i.strip() for i in x if i.strip()]))\n",
    "\n",
    "# Print summary stats with improved formatting\n",
    "note_stats = {\n",
    "    \"Total Patients\": len(note_lengths),\n",
    "    \"Mean Notes/Patient\": f\"{note_lengths.mean():.2f}\",\n",
    "    \"Std Deviation\": f\"{note_lengths.std():.2f}\",\n",
    "    \"Minimum Notes\": int(note_lengths.min()),\n",
    "    \"25th Percentile\": f\"{note_lengths.quantile(0.25):.1f}\",\n",
    "    \"Median Notes\": f\"{note_lengths.median():.1f}\",\n",
    "    \"75th Percentile\": f\"{note_lengths.quantile(0.75):.1f}\",\n",
    "    \"Maximum Notes\": int(note_lengths.max()),\n",
    "    \"Patients w/ Zero Notes\": f\"{(note_lengths == 0).sum()} ({(note_lengths == 0).sum()/len(note_lengths)*100:.1f}%)\"\n",
    "}\n",
    "\n",
    "print(\"\\nüìà SUMMARY STATISTICS:\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in note_stats.items():\n",
    "    print(f\"  {key:<20}: {value}\")\n",
    "\n",
    "# Additional insights\n",
    "total_notes = note_lengths.sum()\n",
    "patients_with_notes = (note_lengths > 0).sum()\n",
    "avg_notes_excluding_zeros = note_lengths[note_lengths > 0].mean() if patients_with_notes > 0 else 0\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Total Notes Across All Patients: {total_notes:,}\")\n",
    "print(f\"  Patients with Notes Available: {patients_with_notes} ({patients_with_notes/len(note_lengths)*100:.1f}%)\")\n",
    "if patients_with_notes > 0:\n",
    "    print(f\"  Avg Notes (excluding zero counts): {avg_notes_excluding_zeros:.2f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "980812ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PTID', 'NOTE_INTERVAL_DD48_NOTES', 'NOTE_TYPE_DD48_NOTES',\n",
      "       'NOTE_SUBTYPE_DD48_NOTES', 'ENCOUNTER_ID_DD48_NOTES',\n",
      "       'DEID_NOTE_TXT_DD48_NOTES'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Reorder every list in each column by ascending order of NOTE_INTERVAL_DD48_NOTES\n",
    "def reorder_notes_by_interval(notes_df):\n",
    "    intervals = notes_df[\"NOTE_INTERVAL_DD48_NOTES\"]\n",
    "    idx_sorted = [np.argsort(interval) for interval in intervals]\n",
    "    reordered = notes_df.copy()\n",
    "    for col in notes_df.columns:\n",
    "        if col == \"PTID\":\n",
    "            continue\n",
    "        reordered[col] = [\n",
    "            [row[i] for i in idx] if isinstance(row, list) and len(row) == len(idx) else row\n",
    "            for row, idx in zip(notes_df[col], idx_sorted)\n",
    "        ]\n",
    "    return reordered\n",
    "\n",
    "NOTES_reordered = reorder_notes_by_interval(NOTES)\n",
    "print(NOTES_reordered.columns)\n",
    "# Combine lists into comma-separated strings for all columns except PTID\n",
    "for col in NOTES_reordered.columns:\n",
    "    if col != \"PTID\":\n",
    "        NOTES_reordered[f\"{col}_str\"] = NOTES_reordered[col].apply(lambda x: \", \".join(map(str, x)) if isinstance(x, list) else x)\n",
    "\n",
    "# Sort by length of string in DEID_NOTE_TXT_DD48_NOTES\n",
    "NOTES_reordered_sorted = NOTES_reordered.sort_values(by=\"DEID_NOTE_TXT_DD48_NOTES_str\", key=lambda x: x.str.len(), ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4888e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìè NOTE CHARACTER LENGTH STATISTICS\n",
      "==================================================\n",
      "üîç Step 1: Found 1451/1451 entries with notes (100.0%)\n",
      "‚è±Ô∏è Step 1 time: 0.00s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ Step 2: Computed character lengths for 1451 entries\n",
      "‚è±Ô∏è Step 2 time: 5.37s\n",
      "\n",
      "üìä Character Length Stats:\n",
      "   Mean:   395855.2\n",
      "   Median: 287123.0\n",
      "   Std:    367090.6\n",
      "   Min:    4,542\n",
      "   Max:    2,818,715\n",
      "   25%:    141392.5\n",
      "   75%:    527567.0\n",
      "‚è±Ô∏è Step 3 time: 0.00s\n",
      "\n",
      "üèÅ Total analysis time: 5.38 seconds\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìè NOTE CHARACTER LENGTH STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract the column\n",
    "notes_column = NOTES_reordered_sorted[\"DEID_NOTE_TXT_DD48_NOTES\"]\n",
    "\n",
    "# Step 1: Data availability check\n",
    "t0 = time.time()\n",
    "total_entries = len(notes_column)\n",
    "non_null_entries = notes_column.notna().sum()\n",
    "print(f\"üîç Step 1: Found {non_null_entries}/{total_entries} entries with notes ({non_null_entries / total_entries * 100:.1f}%)\")\n",
    "print(f\"‚è±Ô∏è Step 1 time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "if non_null_entries == 0:\n",
    "    print(\"‚ö†Ô∏è No non-null entries. Exiting analysis.\")\n",
    "else:\n",
    "    # Step 2: Character length computation\n",
    "    t1 = time.time()\n",
    "    char_lengths = notes_column.dropna().apply(lambda x: len(str(x)))\n",
    "    print(f\"üßÆ Step 2: Computed character lengths for {len(char_lengths)} entries\")\n",
    "    print(f\"‚è±Ô∏è Step 2 time: {time.time() - t1:.2f}s\\n\")\n",
    "\n",
    "    # Step 3: Summary statistics\n",
    "    t2 = time.time()\n",
    "    print(\"üìä Character Length Stats:\")\n",
    "    print(f\"   Mean:   {char_lengths.mean():.1f}\")\n",
    "    print(f\"   Median: {char_lengths.median():.1f}\")\n",
    "    print(f\"   Std:    {char_lengths.std():.1f}\")\n",
    "    print(f\"   Min:    {char_lengths.min():,}\")\n",
    "    print(f\"   Max:    {char_lengths.max():,}\")\n",
    "    print(f\"   25%:    {char_lengths.quantile(0.25):.1f}\")\n",
    "    print(f\"   75%:    {char_lengths.quantile(0.75):.1f}\")\n",
    "    print(f\"‚è±Ô∏è Step 3 time: {time.time() - t2:.2f}s\\n\")\n",
    "\n",
    "print(f\"üèÅ Total analysis time: {time.time() - start_time:.2f} seconds\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1774ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìÖ PATIENT NOTES INTERVAL ANALYSIS\n",
      "==================================================\n",
      "üìä Data: 1447/1451 patients with multiple notes (100%)\n",
      "üìà Spans: Mean=726 days, Median=406 days, Max=3793 days\n",
      "‚è±Ô∏è  Follow-up: Avg=2.0 years, Max=10.4 years\n",
      "üéØ Duration: 66 short-term (‚â§30d), 768 long-term (>1yr)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ANALYSIS OF INTERVALS OF PATIENT NOTES\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìÖ PATIENT NOTES INTERVAL ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "interval_series = NOTES_reordered_sorted[\"NOTE_INTERVAL_DD48_NOTES_str\"].dropna().astype(str)\n",
    "\n",
    "# Convert to lists of floats, ignoring empty values\n",
    "interval_lists = interval_series.apply(\n",
    "    lambda x: [float(i.strip()) for i in x.split(',') if i.strip()]\n",
    ")\n",
    "\n",
    "# Compute max - min (range) per row\n",
    "interval_diffs = interval_lists.apply(lambda x: max(x) - min(x) if len(x) > 1 else 0)\n",
    "\n",
    "# Patient distribution\n",
    "single_note_patients = (interval_lists.apply(len) <= 1).sum()\n",
    "multi_note_patients = (interval_lists.apply(len) > 1).sum()\n",
    "\n",
    "print(f\"üìä Data: {multi_note_patients}/{len(interval_series)} patients with multiple notes ({multi_note_patients/len(interval_series)*100:.0f}%)\")\n",
    "print(f\"üìà Spans: Mean={interval_diffs.mean():.0f} days, Median={interval_diffs.median():.0f} days, Max={interval_diffs.max():.0f} days\")\n",
    "\n",
    "if multi_note_patients > 0:\n",
    "    multi_note_spans = interval_diffs[interval_diffs > 0]\n",
    "    avg_years = multi_note_spans.mean() / 365.25\n",
    "    max_years = interval_diffs.max() / 365.25\n",
    "    \n",
    "    # Time span categories\n",
    "    short_term = (multi_note_spans <= 30).sum()\n",
    "    long_term = (multi_note_spans > 365).sum()\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Follow-up: Avg={avg_years:.1f} years, Max={max_years:.1f} years\")\n",
    "    print(f\"üéØ Duration: {short_term} short-term (‚â§30d), {long_term} long-term (>1yr)\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87fef578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üíä ANTINEOPLASTIC TREATMENT ANALYSIS\n",
      "==================================================\n",
      "üìä Data: 608/1451 patients with treatments (41.9%)\n",
      "üìà Stats: Mean=2.8, Median=3.0, Max=8\n",
      "üéØ Patterns: 93 single (15%), 515 multiple (85%), 22 complex (6+)\n",
      "üíä Total treatments: 1,717\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ANALYSIS OF TREATMENT DATA\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üíä ANTINEOPLASTIC TREATMENT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Data overview\n",
    "total_patients = len(TREATMENT)\n",
    "patients_with_treatment_data = TREATMENT[\"ANTINEOPLASTIC_DD48_TREATMENT\"].notna().sum()\n",
    "\n",
    "# Filter for non-empty treatments\n",
    "nonzero_mask = TREATMENT[\"ANTINEOPLASTIC_DD48_TREATMENT\"].dropna().apply(\n",
    "    lambda x: len([i.strip() for i in x if i.strip()]) > 0\n",
    ")\n",
    "TREATMENT_filtered = TREATMENT.loc[TREATMENT[\"ANTINEOPLASTIC_DD48_TREATMENT\"].notna()]\n",
    "TREATMENT_filtered = TREATMENT_filtered[nonzero_mask]\n",
    "patients_with_actual_treatments = len(TREATMENT_filtered)\n",
    "\n",
    "print(f\"üìä Data: {patients_with_actual_treatments}/{total_patients} patients with treatments ({patients_with_actual_treatments/total_patients*100:.1f}%)\")\n",
    "\n",
    "if patients_with_actual_treatments > 0:\n",
    "    # Count treatments per patient\n",
    "    list_lengths = TREATMENT_filtered[\"ANTINEOPLASTIC_DD48_TREATMENT\"].apply(\n",
    "        lambda x: len([i.strip() for i in x if i.strip()])\n",
    "    )\n",
    "\n",
    "    # Key statistics\n",
    "    print(f\"üìà Stats: Mean={list_lengths.mean():.1f}, Median={list_lengths.median():.1f}, Max={list_lengths.max()}\")\n",
    "    \n",
    "    # Treatment patterns\n",
    "    single = (list_lengths == 1).sum()\n",
    "    multiple = (list_lengths > 1).sum()\n",
    "    complex_cases = (list_lengths > 5).sum()\n",
    "    \n",
    "    print(f\"üéØ Patterns: {single} single ({single/len(list_lengths)*100:.0f}%), {multiple} multiple ({multiple/len(list_lengths)*100:.0f}%), {complex_cases} complex (6+)\")\n",
    "    print(f\"üíä Total treatments: {list_lengths.sum():,}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No treatment data found\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e55413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/patient_notes_raw_split.csv\n"
     ]
    }
   ],
   "source": [
    "notes_ordered_reindexed = NOTES_reordered_sorted.reset_index(drop=False)\n",
    "output_path = \"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/patient_notes_raw_split.csv\"\n",
    "notes_ordered_reindexed.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ee43a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 relevant patient IDs from consistency benchmark files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "df_original = pd.read_csv(\"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/patient_notes_raw.csv\", index_col=0)  # make sure index_col is patient ID\n",
    "df_original.reset_index(inplace=True)  # Reset index to have PTID as a column\n",
    "id_to_ptid_map = df_original[\"PTID\"].to_dict() # Assuming index is patient ID\n",
    "\n",
    "# Step 2: Get consistent and inconsistent patient IDs (from filenames)\n",
    "def get_patient_ids_from_path(path, keyword=\"patient_\", suffix=\".json\"):\n",
    "    return {\n",
    "        fname.split(\".\")[0].replace(keyword, \"\")\n",
    "        for fname in os.listdir(path)\n",
    "        if fname.startswith(keyword) and fname.endswith(suffix)\n",
    "    }\n",
    "\n",
    "consist_path = \"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/old/consistency_bench/consistency_bench_gpt-4.1/consistent\"\n",
    "inconsist_path = \"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/old/consistency_bench/consistency_bench_gpt-4.1/inconsistent\"\n",
    "\n",
    "consist_ids = get_patient_ids_from_path(consist_path)\n",
    "inconsist_ids = get_patient_ids_from_path(inconsist_path)\n",
    "\n",
    "# Step 3: Combine all relevant IDs\n",
    "relevant_ids = consist_ids.union(inconsist_ids)\n",
    "print(f\"Found {len(relevant_ids)} relevant patient IDs from consistency benchmark files.\")\n",
    "relevant_ptids = {id_to_ptid_map[int(pid)] for pid in relevant_ids}\n",
    "\n",
    "df_original.reset_index(inplace=True) \n",
    "filtered_df = df_original[df_original[\"PTID\"].isin(relevant_ptids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e179b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = pd.read_csv(\"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/patient_notes_raw_split.csv\", index_col=0)  # make sure index_col is patient ID\n",
    "filtered_df_split = df_split[df_split[\"PTID\"].isin(relevant_ptids)]\n",
    "assert set(filtered_df[\"PTID\"]) == set(filtered_df_split[\"PTID\"]), \"Mismatch in PTID values between dataframes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "751c3c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2715065/3449612150.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_split[\"DEID_NOTE_TXT_DD48_NOTES\"] = filtered_df_split[\"DEID_NOTE_TXT_DD48_NOTES\"].apply(parse_list_str)\n",
      "/tmp/ipykernel_2715065/3449612150.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_split[\"NOTE_TYPE_DD48_NOTES\"]= filtered_df_split[\"NOTE_TYPE_DD48_NOTES\"].apply(parse_list_str)\n",
      "/tmp/ipykernel_2715065/3449612150.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_split[\"NOTE_INTERVAL_DD48_NOTES\"] = filtered_df_split[\"NOTE_INTERVAL_DD48_NOTES\"].apply(parse_list_str)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def parse_list_str(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return x  # return original if parsing fails\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "filtered_df_split[\"DEID_NOTE_TXT_DD48_NOTES\"] = filtered_df_split[\"DEID_NOTE_TXT_DD48_NOTES\"].apply(parse_list_str)\n",
    "filtered_df_split[\"NOTE_TYPE_DD48_NOTES\"]= filtered_df_split[\"NOTE_TYPE_DD48_NOTES\"].apply(parse_list_str)\n",
    "filtered_df_split[\"NOTE_INTERVAL_DD48_NOTES\"] = filtered_df_split[\"NOTE_INTERVAL_DD48_NOTES\"].apply(parse_list_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1933df4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 LinkedIn patient IDs\n"
     ]
    }
   ],
   "source": [
    "# mercor_ids_consistent = get_patient_ids_from_path(\"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/Mercor/consistent\", \"note_\", \".txt\")\n",
    "    # mercor_ids_inconsistent = get_patient_ids_from_path(\"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/Mercor/inconsistent\", \"note_\", \".txt\")\n",
    "    # mercor_ids = mercor_ids_consistent.union(mercor_ids_inconsistent)\n",
    "    # print(f\"Found {len(mercor_ids)} Mercor patient IDs\")\n",
    "    # centaur_ids = get_patient_ids_from_path(\"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/Centaur/converted_json\", \"patient_\", \".json\")\n",
    "# print(f\"Found {len(centaur_ids)} Centaur patient IDs\")\n",
    "linkedinIDs= get_patient_ids_from_path(\"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn\", \"patient_\", \".txt\")\n",
    "print(f\"Found {len(linkedinIDs)} LinkedIn patient IDs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "581c29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def add_days_to_iso8601(date_str: str, days_to_add: int) -> str:\n",
    "    # Parse the ISO 8601 date string\n",
    "    date_obj = datetime.fromisoformat(date_str)\n",
    "    \n",
    "    # Add days\n",
    "    new_date = date_obj + timedelta(days=days_to_add)\n",
    "    \n",
    "    # Return in ISO 8601 format\n",
    "    return new_date.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a7b7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def replace_date_tokens(text: str, base_date: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace [DATE: <int>] tokens in the text with an actual date based on base_date + int days.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing [DATE: <int>] tokens.\n",
    "        base_date (datetime): The reference date to compute new dates from.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with [DATE: <int>] replaced by actual dates in YYYY-MM-DD format.\n",
    "    \"\"\"\n",
    "    base_date = datetime.fromisoformat(base_date)\n",
    "    def replacer(match):\n",
    "        offset = int(match.group(1))\n",
    "        new_date = base_date + timedelta(days=offset)\n",
    "        return f\"[DATE: {new_date.strftime('%Y-%m-%d')}]\"\n",
    "\n",
    "    return re.sub(r\"\\[DATE:\\s*(-?\\d+)\\]\", replacer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "920740d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID 0 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 1 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 2 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 3 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 4 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 5 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 6 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 7 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 8 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 9 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 10 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 11 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 12 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 13 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 14 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 15 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient W+scHktu18Nt+gNDo24n7DjxjikvU087BmoumjRLoonrFyRdzS9LfUq06b8mhm61 notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_16.json\n",
      "Patient ID 17 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 18 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 19 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 20 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 21 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 22 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 23 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 24 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 25 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 26 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 27 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 28 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 29 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 30 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 31 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 32 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 33 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 34 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 35 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 36 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 37 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 38 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 39 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 40 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 41 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient bc9SXqN/5GtFUfKE9buocz02ihGV/mrzcZVUMzj++ArYLMTnScN8TMZdHw0T3TdH notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_42.json\n",
      "Patient ID 43 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 44 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 45 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 46 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 47 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 48 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 49 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 50 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 51 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 52 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 53 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 54 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 55 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 56 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 57 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 58 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 59 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 60 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 61 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 62 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 63 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 64 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 65 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 66 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 67 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 68 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 69 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 70 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 71 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 72 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 73 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 74 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 75 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 76 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 77 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 78 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 79 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 80 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 81 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 82 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 83 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 84 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 85 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 86 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 87 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 88 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 89 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 90 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 91 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 92 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 93 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 94 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 95 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 96 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 97 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 98 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 99 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 100 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 101 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 102 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 103 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 104 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 105 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 106 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 107 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 108 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 109 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 110 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 111 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 112 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 113 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 114 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 115 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 116 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 117 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 118 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 119 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 120 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 121 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 122 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 123 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 124 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 125 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 126 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 127 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 128 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 129 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 130 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 131 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 132 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 133 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 134 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 135 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 136 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 137 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 138 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 139 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 140 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 141 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 142 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 143 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 144 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 145 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 146 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 147 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 148 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 149 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 150 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 151 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 152 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient 234XtrGZnVegaKv27tVe8F0CNREcYwj3M96Lk7oGtNp6Qs3fO4X9GV0QGZbGRokL notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_153.json\n",
      "Patient ID 154 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 155 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 156 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 157 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 158 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 159 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 160 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 161 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 162 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 163 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient FfFaeqCa2rIqO3bRQtJ0tdo2BrmfOjlVfR1aZzKeOfci52X7LsETnce+rOWFtPLZ notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_164.json\n",
      "Patient ID 165 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 166 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 167 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 168 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 169 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 170 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 171 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 172 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 173 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 174 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 175 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 176 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 177 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 178 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 179 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 180 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 181 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient +FNDMN6GGdzNL1xHjLRC1/6IxkrIEbVD3l3mTA2pKVe51Ff0qtbHLUkyv0yBdcLw notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_182.json\n",
      "Patient ID 183 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 184 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 185 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 186 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 187 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 188 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 189 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 190 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 191 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 192 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 193 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 194 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 195 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 196 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 197 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 198 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 199 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 200 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 201 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient 5eUIUOZaIpJjDCzRmTGau1j7VDLBYsEij7Qwr2wlxGGTCtKCeDyHTHRxAm5jh3OR notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_202.json\n",
      "Saved patient HFaWpAtiIIV5efIcQ959u9YLmafGkcZFO43pcBi+Q4oSBMl6UW7Q/n+aU55nQ9T8 notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_203.json\n",
      "Patient ID 204 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient nDeJVftNyjojdZaXjfgaU18WtDfmgTeKJNk02NOyKroyBJmJOglC9igasroLyv12 notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_205.json\n",
      "Patient ID 206 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient HmCEq3tWSt6UbD1MQ3O6y2qDrHj7xRd3zHWRHFk25j77WwDNcdp3Hc609KRX/Gyn notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_207.json\n",
      "Patient ID 208 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 209 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 210 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 211 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 212 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 213 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 214 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 215 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient T229awyZaY5OhmsgJu/xljocoCvnD0WIzn5gka5/za4djmPHBcNcq05wsEP8c3hR notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_216.json\n",
      "Patient ID 217 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 218 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 219 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 220 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 221 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 222 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 223 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient bTp4kxzjcwbLOWhPB+vjBVd8snAF5F+3L2NyFS0Q9XbnSkfiUfdeHEhd3Z29Zv3U notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_224.json\n",
      "Patient ID 225 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 226 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 227 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 228 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 229 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 230 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 231 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 232 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 233 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 234 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 235 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 236 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 237 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 238 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient 7nC54IkpsjmvLwHcx8fu4rrgn3BGEE2VKolrFbiUCuottYS+5CSwOCCi3E4PqifO notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_239.json\n",
      "Patient ID 240 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient B0Ozcr15HQWiu1Rq+psYGivfRT94DOcgfv7+fBiZ5M50QvDd4F0FEzDg8IfLqIOW notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_241.json\n",
      "Patient ID 242 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 243 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 244 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 245 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 246 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 247 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 248 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 249 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 250 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient 4tkWeZGzZ2PEypmmWFS39Qp5j/JzqtrXTuk5eSpExVPY2sT9MeCm/we8KLP3S4j2 notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_251.json\n",
      "Patient ID 252 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 253 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 254 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient YGtHBL9ubJqVIyMrdi9psmwBb8Fjnjtpxp2iGFfuPEcY7oBl+fmiJQjKsJcQIWUq notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_255.json\n",
      "Patient ID 256 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 257 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 258 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 259 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 260 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 261 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient jZ/IcS56SNPol+r8YHXPPH/4ITjB5Un3IyZyfla3gPXqIrA55wAgNYJQ2DjQueMA notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_262.json\n",
      "Patient ID 263 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 264 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 265 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 266 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 267 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 268 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 269 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 270 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 271 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient yUQnaWKoupqYt6a269uzwYPt7ADZcKwcFsSR2zoXK/pI6cuNg3LBEHMpiP+nuBi0 notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_272.json\n",
      "Patient ID 273 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 274 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 275 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 276 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 277 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 278 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 279 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 280 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 281 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 282 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 283 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 284 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 285 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 286 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 287 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 288 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 289 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 290 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 291 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 292 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 293 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 294 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 295 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 296 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 297 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 298 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 299 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 300 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 301 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 302 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 303 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 304 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 305 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 306 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 307 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 308 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 309 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 310 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 311 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 312 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 313 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 314 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 315 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 316 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 317 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 318 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 319 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 320 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 321 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 322 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 323 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 324 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 325 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 326 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 327 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 328 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 329 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 330 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 331 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 332 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 333 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 334 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 335 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 336 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient mBKK2zcvD9YrTPQQTt5DpIFj0h1NXrY4tbgCmN2wDz3uvgdwV0YgWDBIzhQtUdKz notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_337.json\n",
      "Patient ID 338 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 339 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient 5xwY+IykW0BnA1ETbAwYcLSpVwrgEAhJ0olKvCNodmHu5o+u2LinsA/hSX5n6lXn notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_340.json\n",
      "Patient ID 341 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 342 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 343 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 344 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 345 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 346 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 347 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 348 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient DyWhQ6YHL1D/fDKDxAXbRFwlCJAS7fFCl6ftFsLcsL2brQYHFZQD28B8ko3+9EB9 notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_349.json\n",
      "Patient ID 350 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 351 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 352 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 353 not found in Mercor or Centaur datasets, skipping...\n",
      "Saved patient Hss9YVJVYPaL11jH/PcwKINa3a8JliIwjDNqzxzTFT5E5iMJTmGooWvG7iJZUBug notes to /home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn/patient_354.json\n",
      "Patient ID 355 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 356 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 357 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 358 not found in Mercor or Centaur datasets, skipping...\n",
      "Patient ID 359 not found in Mercor or Centaur datasets, skipping...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filtered_df_split.reset_index(drop=True, inplace=True)  # Reset index to avoid issues with JSON serialization\n",
    "mercor_location = \"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/Mercor/patient_viewer\"\n",
    "centaur_location = \"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/Centaur/patient_viewer\"\n",
    "linkedin_location = \"/home/azureuser/cloudfiles/code/rwep_experiments/alyssa/RLFollow_clean/data/data_labelling/LinkedIn\"\n",
    "mercor_ids=[]\n",
    "centaur_ids=[]\n",
    "for i in range(360):\n",
    "    final_location = None\n",
    "    if str(i) in mercor_ids:\n",
    "        final_location = f\"{mercor_location}/patient_{i}.json\"\n",
    "    elif str(i) in centaur_ids:\n",
    "        final_location = f\"{centaur_location}/patient_{i}.json\"\n",
    "    elif str(i) in linkedinIDs:\n",
    "        final_location = f\"{linkedin_location}/patient_{i}.json\"\n",
    "    else:\n",
    "        print(f\"Patient ID {i} not found in Mercor or Centaur datasets, skipping...\")\n",
    "        continue\n",
    "    patient = filtered_df_split.iloc[i]\n",
    "    index = filtered_df.index[filtered_df['PTID'] == patient['PTID']][0]\n",
    "    assert index == i, f\"Index mismatch for patient {patient['PTID']}: expected {i}, got {index}\"\n",
    "    # Create a JSON object for each patient\n",
    "    total_notes = len(patient[\"DEID_NOTE_TXT_DD48_NOTES\"])\n",
    "    all_jsons = []\n",
    "    date_original = \"1880-01-01\"\n",
    "    for j in range(total_notes):\n",
    "        ptid = patient[\"PTID\"]+\"_\"+str(j)\n",
    "        type = patient[\"NOTE_TYPE_DD48_NOTES\"][j]\n",
    "        date = add_days_to_iso8601(date_original, patient[\"NOTE_INTERVAL_DD48_NOTES\"][j])\n",
    "        text = patient[\"DEID_NOTE_TXT_DD48_NOTES\"][j]\n",
    "        text=replace_date_tokens(text,date_original)\n",
    "        patient_json = {\"id\": ptid, \"type\": type, \"date\": date, \"title\": \"patient notes\", \"text\": text}\n",
    "        all_jsons.append(patient_json)\n",
    "    \n",
    "# Save the JSON objects to a file or process them as needed\n",
    "    # For example, you can save them to a file named \"patient_notes.json\"\n",
    "    with open(final_location, 'w') as json_file:\n",
    "        json.dump(all_jsons, json_file, indent=4)\n",
    "    print(f\"Saved patient {patient['PTID']} notes to {final_location}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1c3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee24ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
